#!/usr/bin/env python3
"""
QualityGate Phase 2 - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
é«˜é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã¨æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
"""

import re
import time
import hashlib
import json
from typing import Dict, List, Tuple, Optional, Set
from pathlib import Path

class PerformanceOptimizer:
    """QualityGateç”¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.compiled_patterns: Dict[str, re.Pattern] = {}
        self.pattern_cache: Dict[str, Tuple[str, str, Dict]] = {}
        self.skip_cache: Set[str] = set()
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'pattern_matches': 0,
            'skip_cache_hits': 0
        }
        
    def get_content_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”¨ï¼‰"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
    
    def is_likely_safe_content(self, content: str) -> bool:
        """æ˜ã‚‰ã‹ã«å®‰å…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é«˜é€Ÿåˆ¤å®š"""
        # ç©ºã¾ãŸã¯çŸ­ã™ãã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        if not content or len(content.strip()) < 3:
            return True
            
        # å±é™ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å®‰å…¨ã¨ã¿ãªã•ãªã„
        danger_keywords = ['sk_', 'pk_', 'api', 'key', 'secret', 'token', 'AKIA', 'rm', 'sudo', 'eval', 'exec', 'ã¨ã‚Šã‚ãˆãš', 'TODO', 'FIXME', 'console.log', 'print']
        content_lower = content.lower()
        for keyword in danger_keywords:
            if keyword.lower() in content_lower:
                return False
            
        # ASCIIæ–‡å­—ã®ã¿ã§æ˜ã‚‰ã‹ã«å®‰å…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        if content.isascii() and len(content) < 50:
            safe_patterns = [
                re.compile(r'^[a-zA-Z0-9\s\-_\.]*$'),  # åŸºæœ¬æ–‡å­—ã®ã¿
                re.compile(r'^(ls|cd|pwd|echo|cat)\s'),  # å®‰å…¨ãªã‚³ãƒãƒ³ãƒ‰
                re.compile(r'^\w+\s*=\s*\w+$'),  # å˜ç´”ãªå¤‰æ•°ä»£å…¥
            ]
            for pattern in safe_patterns:
                if pattern.match(content.strip()):
                    return True
        
        return False
    
    def compile_patterns_optimized(self, patterns: Dict[str, str]) -> Dict[str, re.Pattern]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        compiled = {}
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é•·ã•ã§ã‚½ãƒ¼ãƒˆï¼ˆé•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆï¼‰
        sorted_patterns = sorted(patterns.items(), key=lambda x: len(x[0]), reverse=True)
        
        for pattern_str, message in sorted_patterns:
            try:
                # è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                if len(pattern_str) > 20 or any(char in pattern_str for char in r'.*+?{}[]()^$|\\'):
                    compiled_pattern = re.compile(pattern_str, re.IGNORECASE)
                    compiled[pattern_str] = compiled_pattern
                else:
                    # å˜ç´”ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ–‡å­—åˆ—æ¤œç´¢ã§é«˜é€ŸåŒ–
                    compiled[pattern_str] = pattern_str.lower()
                    
            except re.error:
                # ç„¡åŠ¹ãªæ­£è¦è¡¨ç¾ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
                
        return compiled
    
    def fast_pattern_match(self, content: str, patterns: Dict[str, str]) -> Tuple[Optional[str], Optional[str]]:
        """é«˜é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        content_lower = content.lower()
        
        # ç›´æ¥æ­£è¦è¡¨ç¾ã§ãƒãƒƒãƒãƒ³ã‚°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰
        for pattern_str, message in patterns.items():
            try:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã¾ãŸã¯æ–°ã—ãã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
                if pattern_str not in self.compiled_patterns:
                    self.compiled_patterns[pattern_str] = re.compile(pattern_str, re.IGNORECASE)
                
                compiled_pattern = self.compiled_patterns[pattern_str]
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
                if compiled_pattern.search(content):
                    return pattern_str, message
                    
            except re.error:
                # ç„¡åŠ¹ãªæ­£è¦è¡¨ç¾ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
        
        return None, None
    
    def analyze_with_cache(self, content: str, patterns: Dict[str, str], severity: str) -> Tuple[Optional[str], Optional[str]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãåˆ†æ"""
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        content_hash = self.get_content_hash(content)
        
        # ã‚¹ã‚­ãƒƒãƒ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ã¨åˆ¤å®šã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ï¼‰
        if content_hash in self.skip_cache:
            self.stats['skip_cache_hits'] += 1
            return None, None
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = f"{content_hash}:{severity}"
        if cache_key in self.pattern_cache:
            self.stats['cache_hits'] += 1
            result = self.pattern_cache[cache_key]
            return result[0], result[1]
        
        self.stats['cache_misses'] += 1
        
        # æ˜ã‚‰ã‹ã«å®‰å…¨ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é«˜é€Ÿåˆ¤å®š
        if self.is_likely_safe_content(content):
            self.skip_cache.add(content_hash)
            return None, None
        
        # å®Ÿéš›ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        pattern, message = self.fast_pattern_match(content, patterns)
        
        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        if pattern:
            self.stats['pattern_matches'] += 1
            action = self._get_action_for_severity(severity)
            self.pattern_cache[cache_key] = (pattern, message, action)
            return pattern, message
        else:
            # ãƒãƒƒãƒã—ãªã„å ´åˆã¯é‡è¦åº¦åˆ¥ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å®‰å…¨åˆ¤å®šã®ã¿ï¼‰
            self.pattern_cache[cache_key] = (None, None, {})
            return None, None
    
    def _get_action_for_severity(self, severity: str) -> Dict:
        """é‡è¦åº¦ã«å¿œã˜ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—"""
        actions = {
            'CRITICAL': {'emoji': 'ğŸš«', 'prefix': 'é‡å¤§'},
            'HIGH': {'emoji': 'âš ï¸', 'prefix': 'è­¦å‘Š'},
            'INFO': {'emoji': 'â„¹ï¸', 'prefix': 'æƒ…å ±'}
        }
        return actions.get(severity, {'emoji': 'â“', 'prefix': 'ä¸æ˜'})
    
    def optimize_for_size(self, content: str, max_length: int = 1000) -> str:
        """å¤§ããªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æœ€é©åŒ–"""
        if len(content) <= max_length:
            return content
        
        # é‡è¦ãªéƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆå…ˆé ­ã€æœ«å°¾ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºï¼‰
        important_keywords = [
            'password', 'api', 'key', 'token', 'secret',
            'rm -rf', 'sudo', 'eval', 'exec',
            'ã¨ã‚Šã‚ãˆãš', 'TODO', 'FIXME', 'hack'
        ]
        
        # å…ˆé ­500æ–‡å­—ã¯å¿…ãšå«ã‚ã‚‹
        optimized = content[:500]
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        for keyword in important_keywords:
            idx = content.lower().find(keyword.lower())
            if idx != -1:
                start = max(0, idx - 50)
                end = min(len(content), idx + 50)
                optimized += " " + content[start:end]
        
        # æœ«å°¾200æ–‡å­—ã‚’è¿½åŠ 
        if len(content) > 200:
            optimized += " " + content[-200:]
        
        return optimized[:max_length]
    
    def get_performance_stats(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’å–å¾—"""
        total_queries = self.stats['cache_hits'] + self.stats['cache_misses']
        cache_hit_rate = (self.stats['cache_hits'] / total_queries * 100) if total_queries > 0 else 0
        
        return {
            'cache_hit_rate': f"{cache_hit_rate:.1f}%",
            'total_queries': total_queries,
            'pattern_matches': self.stats['pattern_matches'],
            'skip_cache_hits': self.stats['skip_cache_hits'],
            'compiled_patterns': len(self.compiled_patterns)
        }
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.pattern_cache.clear()
        self.skip_cache.clear()
        self.stats = {key: 0 for key in self.stats.keys()}

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
_optimizer_instance = None

def get_optimizer() -> PerformanceOptimizer:
    """æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _optimizer_instance
    if _optimizer_instance is None:
        _optimizer_instance = PerformanceOptimizer()
    return _optimizer_instance

if __name__ == "__main__":
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
    optimizer = PerformanceOptimizer()
    
    test_patterns = {
        r"(sk|pk)_(test|live)_[0-9a-zA-Z]{24,}": "ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸAPIã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ",
        r"rm\s+-rf\s+/": "å±é™ºãªå†å¸°çš„å‰Šé™¤ã‚³ãƒãƒ³ãƒ‰",
        r"ã¨ã‚Šã‚ãˆãš|æš«å®šå¯¾å¿œ": "ãƒãƒ³ãƒ‰ã‚¨ã‚¤ãƒ‰ä¿®æ­£"
    }
    
    test_content = "sk_live_abc123def456ghi789jkl012mno345"
    
    start_time = time.time()
    for i in range(1000):
        result = optimizer.analyze_with_cache(test_content, test_patterns, "CRITICAL")
    end_time = time.time()
    
    print(f"1000å›å®Ÿè¡Œæ™‚é–“: {(end_time - start_time)*1000:.2f}ms")
    print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ: {optimizer.get_performance_stats()}")