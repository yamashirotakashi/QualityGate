#!/usr/bin/env python3
"""
QualityGate Phase 3A - ç‹¬ç«‹AST Project Analyzer
Python ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ åˆ†æã¨ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³

è¨­è¨ˆåŸå‰‡:
1. å®Œå…¨åˆ†é›¢: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã¸ã®å½±éŸ¿ã‚¼ãƒ­
2. éåŒæœŸå®Ÿè¡Œ: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†
3. è»½é‡è¨­è¨ˆ: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ < 50MB
4. çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤‰æ›´æ™‚ã®ã¿å†åˆ†æ
"""

import ast
import os
import sys
import json
import time
import hashlib
import threading
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor, as_completed

@dataclass
class ProjectMetrics:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    total_files: int = 0
    total_lines: int = 0
    total_functions: int = 0
    total_classes: int = 0
    average_complexity: float = 0.0
    complexity_distribution: Dict[str, int] = None
    import_patterns: Dict[str, int] = None
    naming_patterns: Dict[str, int] = None
    
    def __post_init__(self):
        if self.complexity_distribution is None:
            self.complexity_distribution = {}
        if self.import_patterns is None:
            self.import_patterns = {}
        if self.naming_patterns is None:
            self.naming_patterns = {}

@dataclass
class CodePattern:
    """æ¤œå‡ºã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_type: str  # 'naming', 'import', 'structure', 'complexity'
    pattern: str
    confidence: float
    frequency: int
    examples: List[str]
    suggested_rule: Optional[str] = None

@dataclass
class ProjectAnalysisResult:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æçµæœ"""
    project_path: str
    analysis_timestamp: float
    metrics: ProjectMetrics
    detected_patterns: List[CodePattern]
    suggested_quality_rules: List[Dict[str, str]]
    file_analysis_results: Dict[str, Dict] = None
    
    def __post_init__(self):
        if self.file_analysis_results is None:
            self.file_analysis_results = {}

class ASTProjectAnalyzer:
    """ç‹¬ç«‹AST Project Analyzer - ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œå°‚ç”¨"""
    
    def __init__(self, project_path: str, cache_dir: Optional[str] = None):
        self.project_path = Path(project_path)
        self.cache_dir = Path(cache_dir or self.project_path / ".qualitygate_cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆ†æçŠ¶æ…‹ç®¡ç†
        self.is_analyzing = False
        self.analysis_progress = 0.0
        self.last_analysis_time = 0
        self.analysis_result: Optional[ProjectAnalysisResult] = None
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ¶é™
        self.max_file_size = 100 * 1024  # 100KB
        self.max_files_per_analysis = 1000
        self.analysis_timeout = 30  # 30ç§’
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºè¨­å®š
        self.naming_patterns = {
            'snake_case_function': r'^[a-z_][a-z0-9_]*$',
            'camel_case_class': r'^[A-Z][a-zA-Z0-9]*$',
            'upper_case_constant': r'^[A-Z][A-Z0-9_]*$',
            'private_method': r'^_[a-z_][a-z0-9_]*$'
        }
    
    def get_project_hash(self) -> str:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçŠ¶æ…‹ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”¨ï¼‰"""
        try:
            # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ™‚åˆ»ã‚’é›†è¨ˆ
            python_files = list(self.project_path.rglob("*.py"))
            if not python_files:
                return "empty_project"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨ã‚µã‚¤ã‚ºã®ç°¡æ˜“ãƒãƒƒã‚·ãƒ¥
            total_size = sum(f.stat().st_size for f in python_files if f.exists())
            total_files = len(python_files)
            latest_mtime = max(f.stat().st_mtime for f in python_files if f.exists())
            
            hash_input = f"{total_files}:{total_size}:{latest_mtime}"
            return hashlib.md5(hash_input.encode()).hexdigest()[:16]
            
        except Exception:
            return f"error_{int(time.time())}"
    
    def load_cached_analysis(self) -> Optional[ProjectAnalysisResult]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸåˆ†æçµæœã‚’èª­ã¿è¾¼ã¿"""
        try:
            project_hash = self.get_project_hash()
            cache_file = self.cache_dir / f"analysis_{project_hash}.json"
            
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã«å¾©å…ƒ
                metrics = ProjectMetrics(**data['metrics'])
                patterns = [CodePattern(**p) for p in data['detected_patterns']]
                
                result = ProjectAnalysisResult(
                    project_path=data['project_path'],
                    analysis_timestamp=data['analysis_timestamp'],
                    metrics=metrics,
                    detected_patterns=patterns,
                    suggested_quality_rules=data['suggested_quality_rules'],
                    file_analysis_results=data.get('file_analysis_results', {})
                )
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæ–°ã—ã„å ´åˆã®ã¿ä½¿ç”¨ï¼ˆ24æ™‚é–“ä»¥å†…ï¼‰
                if time.time() - result.analysis_timestamp < 86400:
                    return result
                    
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return None
    
    def save_analysis_cache(self, result: ProjectAnalysisResult):
        """åˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            project_hash = self.get_project_hash()
            cache_file = self.cache_dir / f"analysis_{project_hash}.json"
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‚’è¾æ›¸ã«å¤‰æ›
            data = {
                'project_path': result.project_path,
                'analysis_timestamp': result.analysis_timestamp,
                'metrics': asdict(result.metrics),
                'detected_patterns': [asdict(p) for p in result.detected_patterns],
                'suggested_quality_rules': result.suggested_quality_rules,
                'file_analysis_results': result.file_analysis_results
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def analyze_python_file(self, file_path: Path) -> Optional[Dict]:
        """å˜ä¸€Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if file_path.stat().st_size > self.max_file_size:
                return None
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # ASTè§£æ
            tree = ast.parse(content, filename=str(file_path))
            
            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
            analyzer = FileASTAnalyzer()
            analyzer.visit(tree)
            
            return {
                'file_path': str(file_path.relative_to(self.project_path)),
                'lines_of_code': len(content.splitlines()),
                'functions': analyzer.functions,
                'classes': analyzer.classes,
                'imports': analyzer.imports,
                'complexity_score': analyzer.complexity_score,
                'naming_issues': analyzer.naming_issues
            }
            
        except Exception as e:
            return {
                'file_path': str(file_path.relative_to(self.project_path)),
                'error': str(e),
                'analysis_failed': True
            }
    
    def detect_code_patterns(self, file_results: List[Dict]) -> List[CodePattern]:
        """ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆå¼·åŒ–ç‰ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã‚’ä½¿ç”¨ï¼‰"""
        try:
            # å¼·åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from enhanced_pattern_detector import EnhancedPatternDetector
            
            # å¼·åŒ–æ¤œå‡ºå™¨ã‚’ä½¿ç”¨
            enhanced_detector = EnhancedPatternDetector(str(self.project_path))
            enhanced_patterns = enhanced_detector.enhance_pattern_detection(file_results)
            
            # EnhancedCodePatternã‚’CodePatternã«å¤‰æ›
            patterns = []
            for ep in enhanced_patterns:
                patterns.append(CodePattern(
                    pattern_type=ep.pattern_type.value,
                    pattern=ep.pattern_id,
                    confidence=ep.confidence_score,
                    frequency=ep.frequency,
                    examples=ep.evidence_files[:3],
                    suggested_rule=ep.suggested_message
                ))
            
            return patterns
            
        except ImportError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            return self._detect_basic_patterns(file_results)
    
    def _detect_basic_patterns(self, file_results: List[Dict]) -> List[CodePattern]:
        """åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        patterns = []
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        import_counter = Counter()
        for result in file_results:
            if 'imports' in result:
                for imp in result['imports']:
                    import_counter[imp] += 1
        
        # é »å‡ºã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã—ã¦è¨˜éŒ²
        for import_name, frequency in import_counter.most_common(10):
            if frequency >= 3:  # 3å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                patterns.append(CodePattern(
                    pattern_type='import',
                    pattern=import_name,
                    confidence=min(0.9, frequency / len(file_results)),
                    frequency=frequency,
                    examples=[f"import {import_name}"],
                    suggested_rule=f"æ¨å¥¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {import_name}"
                ))
        
        # å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        function_names = []
        class_names = []
        
        for result in file_results:
            if 'functions' in result:
                function_names.extend(result['functions'])
            if 'classes' in result:
                class_names.extend(result['classes'])
        
        # é–¢æ•°å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        if function_names:
            snake_case_count = sum(1 for name in function_names if name.islower() and '_' in name)
            if snake_case_count / len(function_names) > 0.7:
                patterns.append(CodePattern(
                    pattern_type='naming',
                    pattern='snake_case_functions',
                    confidence=snake_case_count / len(function_names),
                    frequency=snake_case_count,
                    examples=function_names[:3],
                    suggested_rule="é–¢æ•°åã¯snake_caseå½¢å¼ã‚’ä½¿ç”¨"
                ))
        
        # ã‚¯ãƒ©ã‚¹å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        if class_names:
            camel_case_count = sum(1 for name in class_names if name[0].isupper())
            if camel_case_count / len(class_names) > 0.7:
                patterns.append(CodePattern(
                    pattern_type='naming',
                    pattern='camel_case_classes',
                    confidence=camel_case_count / len(class_names),
                    frequency=camel_case_count,
                    examples=class_names[:3],
                    suggested_rule="ã‚¯ãƒ©ã‚¹åã¯CamelCaseå½¢å¼ã‚’ä½¿ç”¨"
                ))
        
        return patterns
    
    def generate_quality_rules(self, patterns: List[CodePattern]) -> List[Dict[str, str]]:
        """å“è³ªãƒ«ãƒ¼ãƒ«è‡ªå‹•ç”Ÿæˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        try:
            # å¼·åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã‹ã‚‰ãƒ«ãƒ¼ãƒ«ç”Ÿæˆæ©Ÿèƒ½ã‚’ä½¿ç”¨
            from enhanced_pattern_detector import EnhancedPatternDetector
            
            enhanced_detector = EnhancedPatternDetector(str(self.project_path))
            enhanced_detector.detected_patterns = []
            
            # CodePatternã‚’EnhancedCodePatternã«å¤‰æ›
            for pattern in patterns:
                if pattern.confidence > 0.7:
                    from enhanced_pattern_detector import EnhancedCodePattern, PatternType
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‚’Enumã«å¤‰æ›
                    try:
                        pattern_type_enum = PatternType(pattern.pattern_type)
                    except ValueError:
                        pattern_type_enum = PatternType.PROJECT_SPECIFIC
                    
                    enhanced_pattern = EnhancedCodePattern(
                        pattern_id=pattern.pattern,
                        pattern_type=pattern_type_enum,
                        pattern_regex=self._generate_regex_for_pattern(pattern),
                        confidence_score=pattern.confidence,
                        frequency=pattern.frequency,
                        evidence_files=pattern.examples,
                        counter_examples=[],
                        suggested_message=pattern.suggested_rule,
                        severity='INFO'
                    )
                    enhanced_detector.detected_patterns.append(enhanced_pattern)
            
            # å¼·åŒ–ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
            return enhanced_detector.generate_project_specific_rules()
            
        except ImportError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
            return self._generate_basic_rules(patterns)
    
    def _generate_regex_for_pattern(self, pattern: CodePattern) -> str:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ­£è¦è¡¨ç¾ã‚’ç”Ÿæˆ"""
        if pattern.pattern_type == 'naming':
            if 'snake_case' in pattern.pattern:
                return r'def\s+[A-Z][a-zA-Z0-9]*\s*\('
            elif 'camel_case' in pattern.pattern:
                return r'class\s+[a-z][a-zA-Z0-9]*\s*[:\(]'
        elif pattern.pattern_type == 'import':
            import re
            return f'import\\s+(?!{re.escape(pattern.pattern)})'
        
        return r'.*'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _generate_basic_rules(self, patterns: List[CodePattern]) -> List[Dict[str, str]]:
        """åŸºæœ¬ãƒ«ãƒ¼ãƒ«ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        rules = []
        
        for pattern in patterns:
            if pattern.confidence > 0.7:  # é«˜ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿
                if pattern.pattern_type == 'import':
                    rules.append({
                        'severity': 'INFO',
                        'pattern': f'import\\s+(?!{pattern.pattern})',
                        'message': f'æ¨å¥¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ {pattern.pattern} ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„',
                        'category': 'project_specific'
                    })
                elif pattern.pattern_type == 'naming':
                    if 'snake_case' in pattern.pattern:
                        rules.append({
                            'severity': 'INFO',
                            'pattern': r'def\s+[A-Z][a-zA-Z0-9]*\s*\(',
                            'message': 'ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯snake_caseé–¢æ•°åãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™',
                            'category': 'naming_convention'
                        })
                    elif 'camel_case' in pattern.pattern:
                        rules.append({
                            'severity': 'INFO',
                            'pattern': r'class\s+[a-z][a-zA-Z0-9]*\s*[:\(]',
                            'message': 'ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯CamelCaseã‚¯ãƒ©ã‚¹åãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™',
                            'category': 'naming_convention'
                        })
        
        return rules
    
    def analyze_project_async(self) -> None:
        """éåŒæœŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼‰"""
        if self.is_analyzing:
            return
        
        def analysis_worker():
            try:
                self.is_analyzing = True
                self.analysis_progress = 0.0
                start_time = time.time()
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
                cached_result = self.load_cached_analysis()
                if cached_result:
                    self.analysis_result = cached_result
                    self.analysis_progress = 1.0
                    self.is_analyzing = False
                    return
                
                # Pythonãƒ•ã‚¡ã‚¤ãƒ«åé›†
                python_files = list(self.project_path.rglob("*.py"))
                python_files = [f for f in python_files 
                              if f.is_file() and not any(part.startswith('.') for part in f.parts)]
                
                if len(python_files) > self.max_files_per_analysis:
                    python_files = python_files[:self.max_files_per_analysis]
                
                if not python_files:
                    self.is_analyzing = False
                    return
                
                # ä¸¦åˆ—ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
                file_results = []
                with ThreadPoolExecutor(max_workers=4) as executor:
                    future_to_file = {
                        executor.submit(self.analyze_python_file, file_path): file_path 
                        for file_path in python_files
                    }
                    
                    for i, future in enumerate(as_completed(future_to_file, timeout=self.analysis_timeout)):
                        result = future.result()
                        if result:
                            file_results.append(result)
                        
                        # é€²æ—æ›´æ–°
                        self.analysis_progress = (i + 1) / len(python_files) * 0.8
                        
                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
                        if time.time() - start_time > self.analysis_timeout:
                            break
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
                detected_patterns = self.detect_code_patterns(file_results)
                self.analysis_progress = 0.9
                
                # å“è³ªãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
                quality_rules = self.generate_quality_rules(detected_patterns)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                total_lines = sum(r.get('lines_of_code', 0) for r in file_results)
                total_functions = sum(len(r.get('functions', [])) for r in file_results)
                total_classes = sum(len(r.get('classes', [])) for r in file_results)
                
                metrics = ProjectMetrics(
                    total_files=len(file_results),
                    total_lines=total_lines,
                    total_functions=total_functions,
                    total_classes=total_classes,
                    average_complexity=sum(r.get('complexity_score', 0) for r in file_results) / len(file_results) if file_results else 0
                )
                
                # çµæœä½œæˆ
                self.analysis_result = ProjectAnalysisResult(
                    project_path=str(self.project_path),
                    analysis_timestamp=time.time(),
                    metrics=metrics,
                    detected_patterns=detected_patterns,
                    suggested_quality_rules=quality_rules,
                    file_analysis_results={r['file_path']: r for r in file_results}
                )
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                self.save_analysis_cache(self.analysis_result)
                
                self.analysis_progress = 1.0
                self.last_analysis_time = time.time()
                
            except Exception as e:
                print(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                self.is_analyzing = False
        
        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        thread = threading.Thread(target=analysis_worker, daemon=True)
        thread.start()
    
    def get_analysis_status(self) -> Dict[str, Any]:
        """åˆ†æçŠ¶æ…‹å–å¾—"""
        return {
            'is_analyzing': self.is_analyzing,
            'progress': self.analysis_progress,
            'last_analysis_time': self.last_analysis_time,
            'has_result': self.analysis_result is not None,
            'cache_dir': str(self.cache_dir)
        }
    
    def get_analysis_result(self) -> Optional[ProjectAnalysisResult]:
        """åˆ†æçµæœå–å¾—"""
        return self.analysis_result
    
    def force_reanalysis(self):
        """å¼·åˆ¶å†åˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        for cache_file in self.cache_dir.glob("analysis_*.json"):
            cache_file.unlink()
        
        self.analysis_result = None
        self.analyze_project_async()

class FileASTAnalyzer(ast.NodeVisitor):
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ASTåˆ†æ"""
    
    def __init__(self):
        self.functions = []
        self.classes = []
        self.imports = []
        self.complexity_score = 0
        self.naming_issues = []
    
    def visit_FunctionDef(self, node):
        self.functions.append(node.name)
        # ç°¡æ˜“è¤‡é›‘åº¦è¨ˆç®—
        self.complexity_score += 1 + len([n for n in ast.walk(node) if isinstance(n, (ast.If, ast.For, ast.While, ast.Try))])
        
        # å‘½åãƒã‚§ãƒƒã‚¯
        if not node.name.islower():
            self.naming_issues.append(f"Function {node.name} is not snake_case")
        
        self.generic_visit(node)
    
    def visit_ClassDef(self, node):
        self.classes.append(node.name)
        
        # å‘½åãƒã‚§ãƒƒã‚¯
        if not node.name[0].isupper():
            self.naming_issues.append(f"Class {node.name} is not CamelCase")
        
        self.generic_visit(node)
    
    def visit_Import(self, node):
        for alias in node.names:
            self.imports.append(alias.name)
        self.generic_visit(node)
    
    def visit_ImportFrom(self, node):
        if node.module:
            for alias in node.names:
                self.imports.append(f"{node.module}.{alias.name}")
        self.generic_visit(node)

def main():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    project_path = "/mnt/c/Users/tky99/dev/qualitygate"
    
    print("ğŸ” AST Project Analyzer ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    analyzer = ASTProjectAnalyzer(project_path)
    
    # åˆ†æé–‹å§‹
    print("ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ†æé–‹å§‹...")
    analyzer.analyze_project_async()
    
    # é€²æ—ç›£è¦–
    while analyzer.is_analyzing:
        status = analyzer.get_analysis_status()
        print(f"é€²æ—: {status['progress']*100:.1f}%")
        time.sleep(1)
    
    # çµæœè¡¨ç¤º
    result = analyzer.get_analysis_result()
    if result:
        print("\nâœ… åˆ†æå®Œäº†!")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {result.metrics.total_files}")
        print(f"ğŸ“ ç·è¡Œæ•°: {result.metrics.total_lines}")
        print(f"ğŸ”§ é–¢æ•°æ•°: {result.metrics.total_functions}")
        print(f"ğŸ“¦ ã‚¯ãƒ©ã‚¹æ•°: {result.metrics.total_classes}")
        print(f"ğŸ§® å¹³å‡è¤‡é›‘åº¦: {result.metrics.average_complexity:.2f}")
        
        print(f"\nğŸ¯ æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(result.detected_patterns)}")
        for pattern in result.detected_patterns[:3]:
            print(f"  â€¢ {pattern.pattern_type}: {pattern.pattern} (ä¿¡é ¼åº¦: {pattern.confidence:.2f})")
        
        print(f"\nğŸ“‹ ç”Ÿæˆãƒ«ãƒ¼ãƒ«æ•°: {len(result.suggested_quality_rules)}")
        for rule in result.suggested_quality_rules[:3]:
            print(f"  â€¢ {rule['severity']}: {rule['message']}")
    else:
        print("âŒ åˆ†æå¤±æ•—")

if __name__ == "__main__":
    main()