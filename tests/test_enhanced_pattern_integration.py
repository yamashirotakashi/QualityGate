#!/usr/bin/env python3
"""
QualityGate Phase 3A Week 2 - çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ†ã‚¹ãƒˆ
ASTåˆ†æå™¨ã¨å¼·åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå™¨ã®çµ±åˆæ¤œè¨¼
"""

import sys
import json
import time
from pathlib import Path

# qualitygate scriptsã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, '/mnt/c/Users/tky99/dev/qualitygate/scripts')

from ast_project_analyzer import ASTProjectAnalyzer
from enhanced_pattern_detector import EnhancedPatternDetector, PatternType

class IntegratedPatternTester:
    """çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ†ã‚¹ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.project_path = "/mnt/c/Users/tky99/dev/qualitygate"
        self.ast_analyzer = ASTProjectAnalyzer(self.project_path)
        self.enhanced_detector = EnhancedPatternDetector(self.project_path)
        
        # ãƒ†ã‚¹ãƒˆç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæŒ‡æ¨™
        self.target_metrics = {
            'pattern_detection_accuracy': 0.85,  # 85%ä»¥ä¸Š
            'rule_generation_accuracy': 0.80,    # 80%ä»¥ä¸Š
            'adaptation_time_seconds': 3.0       # 3ç§’ä»¥ä¸‹
        }
    
    def test_integration_workflow(self) -> dict:
        """çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ”§ çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        start_time = time.time()
        
        # Step 1: ASTåˆ†æå®Ÿè¡Œ
        print("  ğŸ“Š ASTåˆ†æå®Ÿè¡Œ...")
        self.ast_analyzer.analyze_project_async()
        
        # åˆ†æå®Œäº†ã¾ã§å¾…æ©Ÿ
        while self.ast_analyzer.is_analyzing:
            time.sleep(0.5)
        
        result = self.ast_analyzer.get_analysis_result()
        if not result:
            return {'status': 'FAILED', 'reason': 'ASTåˆ†æãŒå¤±æ•—ã—ã¾ã—ãŸ'}
        
        # Step 2: å¼·åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        print("  ğŸ” å¼·åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå®Ÿè¡Œ...")
        file_results = list(result.file_analysis_results.values())
        enhanced_patterns = self.enhanced_detector.enhance_pattern_detection(file_results)
        
        # Step 3: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
        print("  ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ...")
        project_rules = self.enhanced_detector.generate_project_specific_rules()
        
        end_time = time.time()
        adaptation_time = end_time - start_time
        
        return {
            'status': 'SUCCESS',
            'adaptation_time': adaptation_time,
            'detected_patterns': len(enhanced_patterns),
            'generated_rules': len(project_rules),
            'file_analysis_count': len(file_results),
            'patterns': enhanced_patterns,
            'rules': project_rules,
            'project_profile': self.enhanced_detector.project_profile
        }
    
    def test_pattern_accuracy(self, workflow_result: dict) -> dict:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºç²¾åº¦ãƒ†ã‚¹ãƒˆ...")
        
        patterns = workflow_result['patterns']
        
        # æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ã«ï¼‰
        expected_pattern_types = {
            PatternType.NAMING_CONVENTION,
            PatternType.IMPORT_STYLE,
            PatternType.CODE_STRUCTURE,
            PatternType.ERROR_HANDLING,
            PatternType.DOCUMENTATION
        }
        
        detected_pattern_types = {pattern.pattern_type for pattern in patterns}
        
        # ç²¾åº¦è¨ˆç®—
        type_coverage = len(detected_pattern_types & expected_pattern_types) / len(expected_pattern_types)
        
        # ä¿¡é ¼åº¦åˆ†æ
        high_confidence_patterns = [p for p in patterns if p.confidence_score >= 0.8]
        confidence_rate = len(high_confidence_patterns) / len(patterns) if patterns else 0
        
        # ç·åˆç²¾åº¦ã‚¹ã‚³ã‚¢
        overall_accuracy = (type_coverage + confidence_rate) / 2
        
        return {
            'pattern_type_coverage': type_coverage,
            'high_confidence_rate': confidence_rate,
            'overall_accuracy': overall_accuracy,
            'detected_types': [t.value for t in detected_pattern_types],
            'high_confidence_count': len(high_confidence_patterns)
        }
    
    def test_rule_generation_accuracy(self, workflow_result: dict) -> dict:
        """ãƒ«ãƒ¼ãƒ«ç”Ÿæˆç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ“‹ ãƒ«ãƒ¼ãƒ«ç”Ÿæˆç²¾åº¦ãƒ†ã‚¹ãƒˆ...")
        
        rules = workflow_result['rules']
        
        # æœŸå¾…ã•ã‚Œã‚‹ãƒ«ãƒ¼ãƒ«å±æ€§
        required_fields = ['pattern_id', 'severity', 'pattern', 'message', 'category']
        
        valid_rules = []
        for rule in rules:
            if all(field in rule for field in required_fields):
                if rule['confidence'] >= 0.8:  # é«˜ä¿¡é ¼åº¦ãƒ«ãƒ¼ãƒ«
                    valid_rules.append(rule)
        
        rule_validity_rate = len(valid_rules) / len(rules) if rules else 0
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã‚³ãƒ¼ãƒ‰å“è³ªã«é–¢ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã®æ¤œè¨¼
        security_rules = [r for r in valid_rules if 'security' in r['category'].lower()]
        quality_rules = [r for r in valid_rules if 'naming' in r['category'].lower()]
        
        return {
            'rule_validity_rate': rule_validity_rate,
            'valid_rules_count': len(valid_rules),
            'security_rules_count': len(security_rules),
            'quality_rules_count': len(quality_rules),
            'total_rules_generated': len(rules)
        }
    
    def test_performance_targets(self, workflow_result: dict, accuracy_result: dict, rule_result: dict) -> dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™é”æˆãƒ†ã‚¹ãƒˆ"""
        print("ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™é”æˆãƒ†ã‚¹ãƒˆ...")
        
        results = {
            'pattern_detection_accuracy': {
                'target': self.target_metrics['pattern_detection_accuracy'],
                'actual': accuracy_result['overall_accuracy'],
                'passed': accuracy_result['overall_accuracy'] >= self.target_metrics['pattern_detection_accuracy']
            },
            'rule_generation_accuracy': {
                'target': self.target_metrics['rule_generation_accuracy'],
                'actual': rule_result['rule_validity_rate'],
                'passed': rule_result['rule_validity_rate'] >= self.target_metrics['rule_generation_accuracy']
            },
            'adaptation_time': {
                'target': self.target_metrics['adaptation_time_seconds'],
                'actual': workflow_result['adaptation_time'],
                'passed': workflow_result['adaptation_time'] <= self.target_metrics['adaptation_time_seconds']
            }
        }
        
        all_passed = all(result['passed'] for result in results.values())
        
        return {
            'all_targets_met': all_passed,
            'individual_results': results
        }
    
    def generate_integration_report(self) -> dict:
        """çµ±åˆãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("ğŸ“„ çµ±åˆãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ...")
        
        # çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ
        workflow_result = self.test_integration_workflow()
        if workflow_result['status'] != 'SUCCESS':
            return workflow_result
        
        # ç²¾åº¦ãƒ†ã‚¹ãƒˆ
        accuracy_result = self.test_pattern_accuracy(workflow_result)
        rule_result = self.test_rule_generation_accuracy(workflow_result)
        performance_result = self.test_performance_targets(workflow_result, accuracy_result, rule_result)
        
        return {
            'test_timestamp': time.time(),
            'overall_status': 'PASSED' if performance_result['all_targets_met'] else 'FAILED',
            'workflow': workflow_result,
            'pattern_accuracy': accuracy_result,
            'rule_generation': rule_result,
            'performance_validation': performance_result,
            'project_profile': workflow_result.get('project_profile')
        }

def main():
    """çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ QualityGate Phase 3A Week 2 çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    tester = IntegratedPatternTester()
    
    # çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    report = tester.generate_integration_report()
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“‹ çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("-" * 40)
    
    print(f"ğŸ“Š ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {report['overall_status']}")
    
    if report['overall_status'] == 'PASSED':
        workflow = report['workflow']
        accuracy = report['pattern_accuracy']
        rules = report['rule_generation']
        performance = report['performance_validation']
        
        print(f"\nğŸ”§ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµæœ:")
        print(f"  é©å¿œæ™‚é–“: {workflow['adaptation_time']:.2f}ç§’")
        print(f"  æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {workflow['detected_patterns']}")
        print(f"  ç”Ÿæˆãƒ«ãƒ¼ãƒ«æ•°: {workflow['generated_rules']}")
        print(f"  åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {workflow['file_analysis_count']}")
        
        print(f"\nğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºç²¾åº¦:")
        print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‚«ãƒãƒ¬ãƒƒã‚¸: {accuracy['pattern_type_coverage']*100:.1f}%")
        print(f"  é«˜ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ç‡: {accuracy['high_confidence_rate']*100:.1f}%")
        print(f"  ç·åˆç²¾åº¦: {accuracy['overall_accuracy']*100:.1f}%")
        
        print(f"\nğŸ“‹ ãƒ«ãƒ¼ãƒ«ç”Ÿæˆç²¾åº¦:")
        print(f"  æœ‰åŠ¹ãƒ«ãƒ¼ãƒ«ç‡: {rules['rule_validity_rate']*100:.1f}%")
        print(f"  ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«æ•°: {rules['security_rules_count']}")
        print(f"  å“è³ªãƒ«ãƒ¼ãƒ«æ•°: {rules['quality_rules_count']}")
        
        print(f"\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™é”æˆçŠ¶æ³:")
        for metric, result in performance['individual_results'].items():
            status = "âœ… é”æˆ" if result['passed'] else "âŒ æœªé”æˆ"
            print(f"  {metric}: {status} ({result['actual']:.2f} / {result['target']:.2f})")
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º
        if report.get('project_profile'):
            profile = report['project_profile']
            print(f"\nğŸ—ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:")
            print(f"  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—: {profile.project_type}")
            print(f"  ä¸»è¦ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: {', '.join(profile.primary_frameworks)}")
            print(f"  è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«: {profile.complexity_level}")
            print(f"  ãƒãƒ¼ãƒ è¦æ¨¡æ¨å®š: {profile.team_size_indicator}")
            
    else:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {report.get('reason', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
    
    # è©³ç´°çµæœã‚’JSONã§ä¿å­˜
    report_path = Path("/mnt/c/Users/tky99/dev/qualitygate/test_results/integration_test_report.json")
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        serializable_report = json.loads(json.dumps(report, default=str))
        json.dump(serializable_report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    
    return report['overall_status'] == 'PASSED'

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)